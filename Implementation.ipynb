{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cba7d10",
   "metadata": {},
   "source": [
    "# Project Implementation\n",
    "## Implementation for the Lecture Summarizer\n",
    "This Jupyter Notebook contains the relevant code for the implementation of this project. All techniques and models are grouped under their respective headers. You can directly change any input parameters when running the cells including the text you wish to summarize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad897831",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The Lecture Summarizer Project is a Natural Language Processing (NLP) project that aims to summarize lectures automatically using various text summarization techniques. \n",
    "\n",
    "## Text Summarization Techniques\n",
    "\n",
    "Text summarization techniques can be divided into three categories:\n",
    "\n",
    "1. **Extractive**: This technique extracts the most important sentences or phrases from the original text to create a summary. It is widely used due to its simplicity and effectiveness.\n",
    "\n",
    "2. **Abstractive**: This technique generates a summary by paraphrasing and rephrasing the original text. It is more challenging than extractive summarization, but it can generate summaries that are more concise and coherent.\n",
    "\n",
    "3. **Descriptive**: This technique provides a description of the main topics and ideas discussed in the original text. It is less common than the other two techniques but can be useful for certain applications.\n",
    "\n",
    "## Transformers\n",
    "\n",
    "Transformers are a type of neural network architecture that has revolutionized NLP in recent years. Some popular transformer models used in text summarization are:\n",
    "\n",
    "1. **T5**: T5 (Text-to-Text Transfer Transformer) is a transformer model that can perform a wide range of NLP tasks, including text summarization. It has achieved state-of-the-art performance in many NLP benchmarks.\n",
    "\n",
    "2. **BERT**: BERT (Bidirectional Encoder Representations from Transformers) is another transformer model that has shown excellent results in various NLP tasks, including text summarization.\n",
    "\n",
    "3. **Longformer Encoder-Decoder (LED)**: This is a transformer model that can handle long sequences of text, which is important for text summarization since it often involves processing large amounts of text.\n",
    "\n",
    "## Relevant Information\n",
    "\n",
    "The Lecture Summarizer Project can have various applications, such as helping students to study more efficiently, enabling researchers to scan through a large number of papers quickly, and assisting professionals to prepare for meetings and presentations. However, developing an accurate and reliable lecture summarizer is still a challenging task, and researchers are continuously working on improving the existing techniques and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21abc9b5",
   "metadata": {},
   "source": [
    "## Abstractive Text Summarization (Base Models)\n",
    "This cell contains the implmentation of abstractive text summarization using the Base T5 and BART Transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6706879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Model:\n",
      "       1. BART \n",
      "       2. T5\n",
      "Enter Model Number:1\n",
      "ENTER TEXT TO SUMMARIZE HERE:BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning model that has been widely used in natural language processing (NLP) tasks. BERT was introduced in a 2018 paper by researchers at Google, and it has since become one of the most popular and widely used NLP models.  BERT is characterized by its ability to generate contextualized word embeddings, which are vector representations of words in a text that capture their meaning in context. Unlike traditional word embeddings that have a fixed representation for each word, contextualized word embeddings can change depending on the context in which the word appears. This allows BERT to better capture the nuances and complexities of natural language.  BERT is pre-trained on large amounts of text data using an unsupervised learning technique called masked language modeling. During training, a certain percentage of the words in the input text are masked, and the model is trained to predict the masked words based on the surrounding context. This process allows the model to learn to represent the meaning of words in the context of the surrounding text.  After pre-training, BERT can be fine-tuned on specific downstream NLP tasks such as sentiment analysis, text classification, and question answering. Fine-tuning involves training the model on a smaller dataset specific to the target task, allowing the model to adapt to the specific nuances and complexities of the task.  Overall, BERT has had a significant impact on NLP and has achieved state-of-the-art results on many benchmark NLP tasks. Its ability to generate contextualized word embeddings has made it particularly effective in tasks that require understanding the meaning of text in context.\n",
      "\n",
      "\n",
      "\n",
      "***Summary***\n",
      "BERT is a transformer-based machine learning model that has been widely used in natural language processing (NLP) tasks . its ability to generate contextualized word embeddings has made it particularly effective in tasks that require understanding the meaning of text in context .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "print(\"\"\"Select Model:\n",
    "       1. BART \n",
    "       2. T5\"\"\")\n",
    "model = input(\"Enter Model Number:\")\n",
    "\n",
    "if model == '1':\n",
    "    _num_beams = 4\n",
    "    _no_repeat_ngram_size = 3\n",
    "    _length_penalty = 1\n",
    "    _min_length = 12\n",
    "    _max_length = 128\n",
    "    _early_stopping = True\n",
    "else:\n",
    "    _num_beams = 4\n",
    "    _no_repeat_ngram_size = 3\n",
    "    _length_penalty = 2\n",
    "    _min_length = 30\n",
    "    _max_length = 200\n",
    "    _early_stopping = True\n",
    "\n",
    "text = \"\"\"***ENTER TEXT TO SUMMARIZE HERE***\"\"\"\n",
    "text = input(\"ENTER TEXT TO SUMMARIZE HERE:\")\n",
    "\n",
    "def run_model(input_text):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if model == \"BART\":\n",
    "        bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "        bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "        input_text = str(input_text)\n",
    "        input_text = ' '.join(input_text.split())\n",
    "        input_tokenized = bart_tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "        summary_ids = bart_model.generate(input_tokenized,\n",
    "                                          num_beams=_num_beams,\n",
    "                                          no_repeat_ngram_size=_no_repeat_ngram_size,\n",
    "                                          length_penalty=_length_penalty,\n",
    "                                          min_length=_min_length,\n",
    "                                          max_length=_max_length,\n",
    "                                          early_stopping=_early_stopping)\n",
    "\n",
    "        output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in\n",
    "                  summary_ids]\n",
    "        print('\\n\\n\\n***Summary***')\n",
    "        print(output[0])\n",
    "\n",
    "    else:\n",
    "        t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "        t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "        input_text = str(input_text).replace('\\n', '')\n",
    "        input_text = ' '.join(input_text.split())\n",
    "        input_tokenized = t5_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "        summary_task = torch.tensor([[21603, 10]]).to(device)\n",
    "        input_tokenized = torch.cat([summary_task, input_tokenized], dim=-1).to(device)\n",
    "        summary_ids = t5_model.generate(input_tokenized,\n",
    "                                        num_beams=_num_beams,\n",
    "                                        no_repeat_ngram_size=_no_repeat_ngram_size,\n",
    "                                        length_penalty=_length_penalty,\n",
    "                                        min_length=_min_length,\n",
    "                                        max_length=_max_length,\n",
    "                                        early_stopping=_early_stopping)\n",
    "        output = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in\n",
    "                  summary_ids]\n",
    "        print('\\n\\n\\n***Summary***')\n",
    "        print(output[0])\n",
    "if text!=\"\"\"***ENTER TEXT TO SUMMARIZE HERE***\"\"\":\n",
    "    run_model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1edc43",
   "metadata": {},
   "source": [
    "## Abstractive Text Summarization (Fine-Tuned Models)\n",
    "This cell contains the implmentation of abstractive text summarization using the T5-Long, LED-Base and LED-Long Transformer models fine-tuned on the BookSum dataset to generate abstractive and descriptive text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a0133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Model:\n",
      "       1. Long T5\n",
      "       2. LED Base\n",
      "       3. LED Long\n",
      "Enter Model Number:2\n",
      "Minimum Length:16\n",
      "Maximum Length:256\n",
      "Number of Beams:4\n",
      "Repetition Penalty:3.5\n",
      "N-Gram Repeats:3\n",
      "Encoder N-Gram Repeats:3\n",
      "ENTER TEXT TO SUMMARIZE HERE:BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning model that has been widely used in natural language processing (NLP) tasks. BERT was introduced in a 2018 paper by researchers at Google, and it has since become one of the most popular and widely used NLP models.  BERT is characterized by its ability to generate contextualized word embeddings, which are vector representations of words in a text that capture their meaning in context. Unlike traditional word embeddings that have a fixed representation for each word, contextualized word embeddings can change depending on the context in which the word appears. This allows BERT to better capture the nuances and complexities of natural language.  BERT is pre-trained on large amounts of text data using an unsupervised learning technique called masked language modeling. During training, a certain percentage of the words in the input text are masked, and the model is trained to predict the masked words based on the surrounding context. This process allows the model to learn to represent the meaning of words in the context of the surrounding text.  After pre-training, BERT can be fine-tuned on specific downstream NLP tasks such as sentiment analysis, text classification, and question answering. Fine-tuning involves training the model on a smaller dataset specific to the target task, allowing the model to adapt to the specific nuances and complexities of the task.  Overall, BERT has had a significant impact on NLP and has achieved state-of-the-art results on many benchmark NLP tasks. Its ability to generate contextualized word embeddings has made it particularly effective in tasks that require understanding the meaning of text in context.\n",
      "\n",
      "\n",
      "\n",
      "***Summary***\n",
      "This paper describes the use of BERT, a transformational machine learning (machine learning) model that can automatically predict and predict words in \"context\" . In particular, it is able to predict masked words in text that are difficult to interpret because they cannot always be predicted. This makes BERT particularly useful in tasks requiring understanding the complexities of language.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "#define model maps\n",
    "model_map={'Long T5':'pszemraj/long-t5-tglobal-base-16384-book-summary',\n",
    "           'LED Base':'pszemraj/led-base-book-summary',\n",
    "           'LED Long':'pszemraj/led-large-book-summary'\n",
    "           }\n",
    "\n",
    "#Select the model to be used for the summary\n",
    "print(\"\"\"Select Model:\n",
    "       1. Long T5\n",
    "       2. LED Base\n",
    "       3. LED Long\"\"\")\n",
    "model = input(\"Enter Model Number:\")\n",
    "\n",
    "if model == '1':\n",
    "    model = 'Long T5'\n",
    "elif model == '2':\n",
    "    model = 'LED Base'\n",
    "else:\n",
    "    model = 'LED Long'\n",
    "\n",
    "#Slider to control the model hyperparameter\n",
    "_min_length = input(\"Minimum Length:\")\n",
    "_max_length = input(\"Maximum Length:\")\n",
    "_num_beams = input(\"Number of Beams:\")\n",
    "_repetition_penalty = float(input(\"Repetition Penalty:\"))\n",
    "_no_repeat_ngram_size = input(\"N-Gram Repeats:\")\n",
    "_encoder_no_repeat_ngram_size = input(\"Encoder N-Gram Repeats:\")\n",
    "        \n",
    "#Provide the input text to be summarized\n",
    "text = input(\"ENTER TEXT TO SUMMARIZE HERE:\")\n",
    "\n",
    "#write a run_model function that runs the program\n",
    "def run_model(input_text):\n",
    "    global model\n",
    "    hf_name = model_map[model]\n",
    "    #initialize the summarizer in pipeline\n",
    "    if model == 'Long T5':\n",
    "        summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            hf_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "        result = summarizer(\n",
    "           input_text,\n",
    "           min_length=int(_min_length), \n",
    "           max_length=int(_max_length),\n",
    "           no_repeat_ngram_size=int(_no_repeat_ngram_size), \n",
    "           encoder_no_repeat_ngram_size=int(_encoder_no_repeat_ngram_size),\n",
    "           repetition_penalty=_repetition_penalty,\n",
    "           num_beams=int(_num_beams),\n",
    "           do_sample=False,\n",
    "           early_stopping=True\n",
    "        )\n",
    "        summary=result[0]['summary_text']\n",
    "        print('\\n\\n\\n***Summary***')\n",
    "        print(summary)\n",
    "    elif model == 'LED Base':\n",
    "        summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            hf_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "        result = summarizer(\n",
    "           input_text,\n",
    "           min_length=int(_min_length), \n",
    "           max_length=int(_max_length),\n",
    "           no_repeat_ngram_size=int(_no_repeat_ngram_size), \n",
    "           encoder_no_repeat_ngram_size=int(_encoder_no_repeat_ngram_size),\n",
    "           repetition_penalty=_repetition_penalty,\n",
    "           num_beams=int(_num_beams),\n",
    "           do_sample=False,\n",
    "           early_stopping=True\n",
    "        )\n",
    "        summary=result[0]['summary_text']\n",
    "        print('\\n\\n\\n***Summary***')\n",
    "        print(summary)\n",
    "    else:\n",
    "        summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            hf_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "        result = summarizer(\n",
    "           input_text,\n",
    "           min_length=int(_min_length), \n",
    "           max_length=int(_max_length),\n",
    "           no_repeat_ngram_size=int(_no_repeat_ngram_size), \n",
    "           encoder_no_repeat_ngram_size=int(_encoder_no_repeat_ngram_size),\n",
    "           repetition_penalty=_repetition_penalty,\n",
    "           num_beams=int(_num_beams),\n",
    "           early_stopping=True\n",
    "        )\n",
    "        summary=result[0]['summary_text']\n",
    "        print('\\n\\n\\n***Summary***')\n",
    "        print(summary)\n",
    "    \n",
    "\n",
    "#Creating button for execute the text summarization\n",
    "if text:\n",
    "    run_model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f5d6a",
   "metadata": {},
   "source": [
    "## Extractive Text Summarization\n",
    "This cell contains the implmentation of extractive text summarization using the BERT Transformer models fine-tuned on the CNN-DM dataset to generate extractive text. This library is based on dmmiller612's lecture-summarizer repo of which it is a generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84029ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTER TEXT TO SUMMARIZE HERE:BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning model that has been widely used in natural language processing (NLP) tasks. BERT was introduced in a 2018 paper by researchers at Google, and it has since become one of the most popular and widely used NLP models.  BERT is characterized by its ability to generate contextualized word embeddings, which are vector representations of words in a text that capture their meaning in context. Unlike traditional word embeddings that have a fixed representation for each word, contextualized word embeddings can change depending on the context in which the word appears. This allows BERT to better capture the nuances and complexities of natural language.  BERT is pre-trained on large amounts of text data using an unsupervised learning technique called masked language modeling. During training, a certain percentage of the words in the input text are masked, and the model is trained to predict the masked words based on the surrounding context. This process allows the model to learn to represent the meaning of words in the context of the surrounding text.  After pre-training, BERT can be fine-tuned on specific downstream NLP tasks such as sentiment analysis, text classification, and question answering. Fine-tuning involves training the model on a smaller dataset specific to the target task, allowing the model to adapt to the specific nuances and complexities of the task.  Overall, BERT has had a significant impact on NLP and has achieved state-of-the-art results on many benchmark NLP tasks. Its ability to generate contextualized word embeddings has made it particularly effective in tasks that require understanding the meaning of text in context.\n",
      "Minimum Sentence Length:40\n",
      "Maximum Sentence Length:256\n",
      "Reduction Ratio (0-1):0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/student/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p5/g1xglqt57k33nv_q0c54zcm80000gn/T/ipykernel_1385/575807595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#Creating button for execute the text summarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/p5/g1xglqt57k33nv_q0c54zcm80000gn/T/ipykernel_1385/575807595.py\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_min_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_max_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\n***Summary***'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/summarizer/summary_processor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, body, ratio, min_length, max_length, use_first, algorithm, num_sentences, return_as_list)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0msummary\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         return self.run(body, ratio, min_length, max_length,\n\u001b[0m\u001b[1;32m    235\u001b[0m                         use_first, algorithm, num_sentences, return_as_list)\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/summarizer/summary_processor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, body, ratio, min_length, max_length, use_first, algorithm, num_sentences, return_as_list)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_as_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/summarizer/summary_processor.py\u001b[0m in \u001b[0;36mcluster_runner\u001b[0;34m(self, sentences, ratio, algorithm, use_first, num_sentences)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         summary_sentence_indices = ClusterFeatures(\n\u001b[0m\u001b[1;32m    120\u001b[0m             hidden, algorithm, random_state=self.random_state).cluster(ratio, num_sentences)\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/summarizer/cluster_features.py\u001b[0m in \u001b[0;36mcluster\u001b[0;34m(self, ratio, num_sentences)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_centroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m             \u001b[0mkmeans_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kmeans_single_lloyd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_mkl_vcomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \u001b[0mbest_inertia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_mkl_vcomp\u001b[0;34m(self, X, n_samples)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0mn_active_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mCHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_active_threads\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreadpool_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m             \u001b[0mhas_vcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"vcomp\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             has_mkl = (\"mkl\", \"intel\") in [\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36mthreadpool_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreadpool_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\u001b[0m in \u001b[0;36mthreadpool_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mIn\u001b[0m \u001b[0maddition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0minternal_api\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ThreadpoolInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_api\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_ALL_USER_APIS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_if_incompatible_openmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m_load_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;34m\"\"\"Loop through loaded libraries and store supported ones\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"darwin\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_modules_with_dyld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"win32\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_modules_with_enum_process_module_ex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m_find_modules_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Store the module if it is supported and selected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_module_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_find_modules_with_enum_process_module_ex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m_make_module_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefixes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0muser_api\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0mmodule_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minternal_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_RTLD_NOLOAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_threads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_extra_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\u001b[0m in \u001b[0;36mget_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    644\u001b[0m                              lambda: None)\n\u001b[1;32m    645\u001b[0m         \u001b[0mget_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"OpenBLAS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "### This module is presently facing an error due to issues with access of oython libraries in macos\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "#Provide the input area for text to be summarized\n",
    "text = input(\"ENTER TEXT TO SUMMARIZE HERE:\")\n",
    "\n",
    "# Take parameters as input\n",
    "_min_length = int(input(\"Minimum Sentence Length:\"))\n",
    "_max_length = int(input(\"Maximum Sentence Length:\"))\n",
    "_ratio = float(input(\"Reduction Ratio (0-1):\"))\n",
    "\n",
    "def run_model(input_text):\n",
    "    model = Summarizer()\n",
    "    result = model(input_text, min_length=_min_length,max_length=_max_length,ratio=_ratio)\n",
    "    summary = ''.join(result)\n",
    "    print('\\n\\n\\n***Summary***')\n",
    "    print(summary)\n",
    "\n",
    "#Creating button for execute the text summarization\n",
    "if text:\n",
    "    run_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826b3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
